{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant faire une prédiction et dire si une phrase est affirmative ou négative ! \n",
    "pour cela on utilise une dataset de puis imdb ! \n",
    "nous allons commencer par lire dataset   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (imdb.py, line 42)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\TRIUMPHAL\\AppData\\Local\\conda\\conda\\envs\\TensorProject\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2963\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-e2e6251147ef>\"\u001b[1;36m, line \u001b[1;32m7\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import imdb\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\TRIUMPHAL\\Exercices\\imdb.py\"\u001b[1;36m, line \u001b[1;32m42\u001b[0m\n\u001b[1;33m    data_dir = \"\\aclImdb_v1\\aclImdb\\\"\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import time \n",
    "from scipy.spatial.distance import cdist\n",
    "#from tensorflow.keras.datasets import imdb\n",
    "import imdb \n",
    "\n",
    "#  pour le modéle  ! \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  0\n",
      "Test-set size:   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TRIUMPHAL\\AppData\\Local\\conda\\conda\\envs\\TensorProject\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\TRIUMPHAL\\AppData\\Local\\conda\\conda\\envs\\TensorProject\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-447def93efa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m#et donx le maximum de tokens (mots) dans notre cas est\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TensorProject\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[1;32m-> 2272\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TensorProject\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# small reductions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "x_train_text, y_train = imdb.load_data(train=True)\n",
    "x_test_text, y_test = imdb.load_data(train=False)\n",
    "\n",
    "print(\"Train-set size: \", len(x_train_text))\n",
    "print(\"Test-set size:  \", len(x_test_text))\n",
    "\n",
    "data_text = x_train_text + x_test_text\n",
    "\n",
    " # nous allons maintenant  transformer notre data en tokens ! \n",
    "num_words=10000\n",
    "tokenizer=Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "\n",
    "# maintenat nous devons utiliser le tokenizers pour rendre nis x_train_text en x_train_tokens\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)#texts_to_sequences fait l'affaire! \n",
    "x_test_tokens= tokenizer.texts_to_sequences(x_test_text)\n",
    "\n",
    "\n",
    "#on vient de normaliser nos données maintenant nous devons les rendre equilibrés ! de sorte que les input aurons la meme taille *\n",
    "#pour cela nous devons maximiser le nombre de tokens ! \n",
    "#calculons le max \n",
    "#ici nous prenons tous les tokens ! (test+train) et ceci pour chaque longueur de token ! et le rendre un array puis voir \n",
    "#la moyenne de celui si avec np.main(num_tokens)\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "np.mean(num_tokens)\n",
    "\n",
    "np.max(num_tokens)\n",
    "#et donx le maximum de tokens (mots) dans notre cas est \n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)#moyenne+2*l'ecart type ! \n",
    "max_tokens = int(max_tokens)#partie entriére \n",
    "max_tokens #la taille max  de la sequence est donc : 545 \n",
    "tokenizer.word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maintenant en ce lance au padding et trancking ! alors aprés avoir peut etre alarger notre token  ! il faut remplir les vides avec des 0 mais est ce que au debut ! ou bien à la fin  ! si pad=pred (au debut) sinon si pad=post alors à la fin! \n",
    "on choisit avant ! \n",
    "pad=pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad=\"pre\"\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad) #et voila du x_train_text => x_train_tokens=>x_train_pad \n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "\n",
    "x_train_pad.shape\n",
    "x_test_pad.shape\n",
    "#pour bien comprendre voila un affichage : \n",
    "x_train_pad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion  token-integer =>words\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation the model  !\n",
    "\n",
    "model=Sequential()\n",
    "embedding_size=8\n",
    "couche_1 = Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                     name='layer_embedding')\n",
    "#cette couche est obligatoirement la premiére du model ! #je pense que c est equivalant a les scaller ! \n",
    "#Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "model.add(couche_1)\n",
    "\n",
    "couche_2=LSTM(32,activation='tanh',return_sequences=True)\n",
    "model.add(couche_2)\n",
    "\n",
    "couche_3=LSTM(16,activation='tanh',return_sequences=True)\n",
    "model.add(couche_3)\n",
    "\n",
    "couche_3=LSTM(8,activation='tanh')\n",
    "model.add(couche_3)\n",
    "\n",
    "couche_4=Dense(1, activation='sigmoid')\n",
    "model.add(couche_4)\n",
    "\n",
    "#compile : \n",
    "optimizer=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64) #training\n",
    "result = model.evaluate(x_test_pad, y_test)#testing \n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(result[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8]\n",
    "tokens = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "tokens_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(tokens_pad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
